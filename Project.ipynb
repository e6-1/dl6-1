{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autonomous Braking\n",
    "\n",
    "## Ethan Petersen, Josh Laesch, Ben Wong\n",
    "\n",
    "### The Dream Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as pylab\n",
    "import imageio\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imageio.core.util import asarray as imgToArr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "We perform some preprocessing on the data before feeding into the models. Namely, we will split the data into training, validation, and tests sets that will have equivalent amounts of braking and nonbraking frames. Note: frames 80,000 to 100,000 correspond to congested city center traffic following a large truck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "videoFile = './data/driving.avi'\n",
    "vid = imageio.get_reader(videoFile,  'ffmpeg')\n",
    "\n",
    "# Columns: Frame, Brake, GazeX, GazeY\n",
    "dataFile = './data/cleaned_data.csv'\n",
    "df = pd.read_csv(dataFile, delimiter='\\t')\n",
    "\n",
    "brake = df[df['Brake'] > 0]\n",
    "nonbrake = df[df['Brake'] == 0]\n",
    "nonbrake = nonbrake[:len(brake)]  # Braking is far fewer than nonbraking, so trim down\n",
    "df = pd.concat([brake, nonbrake])\n",
    "df = df.drop(df[df['GazeX'] < 0].index)\n",
    "df = df.drop(df[df['GazeY'] < 0].index)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)  # Resets the index to the usual 0, 1, 2, ...\n",
    "\n",
    "# One-hot encode brakes\n",
    "outputs = OneHotEncoder(sparse=False).fit_transform(df['Brake'].reshape(-1,1))  # column 0: no brake, column 1: brake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In training, we sometimes pull straight from the video and specify a stride length\n",
    "# but we have also stored 28x28x3 glimpses for each frame\n",
    "input_glimpses = np.zeros((80000, 28, 28, 3))\n",
    "input_gazes = np.zeros((80000, 2))\n",
    "outputs = np.zeros((80000, 2))\n",
    "for batch in range(1, 9):\n",
    "    file_name = \"data/glimpse_batchc_{0}.npz\".format(batch)\n",
    "    array = np.load(file_name)\n",
    "    input_glimpses[(batch - 1) * 10000: batch * 10000] = array['frames']\n",
    "    input_gazes[(batch - 1) * 10000: batch * 10000] = array['gazes']\n",
    "    outputs[(batch - 1) * 10000: batch * 10000] = array['braking']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper methods\n",
    "We define some helper functions: `minibatch` will get batches of data for training via SGD, `get_glimpses` processes a list of images and cuts out small glimpses based on a list of center coordinates, and `get_glimpse` performs the cutting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def minibatch(data, batch_size, data_size):\n",
    "    \"\"\"Generates a minibatch from the given data and parameters.\"\"\"\n",
    "    randomized = np.random.permutation(data)\n",
    "    batches = []\n",
    "    num_batches = 0\n",
    "    while num_batches * batch_size < data_size:\n",
    "        new_batch = randomized[num_batches * batch_size:(num_batches + 1) * batch_size]\n",
    "        batches.append(new_batch)\n",
    "        num_batches += 1\n",
    "    return batches\n",
    "\n",
    "def get_glimpses(images, coords):\n",
    "    \"\"\"Gets a batch of glimpses.\"\"\"\n",
    "    arr = []\n",
    "    for img, coord in zip(images, coords):\n",
    "        arr.append(get_glimpse(img, coord[0], coord[1]))\n",
    "    return np.array(arr)\n",
    "\n",
    "def get_glimpse(image, x, y, stride=14):\n",
    "    \"\"\"Returns a subsection (glimpse) of the image centered on the given point.\"\"\"\n",
    "    x = int(x)  # Force to int\n",
    "    y = int(y)  # Force to int\n",
    "    min_x = x - stride\n",
    "    max_x = x + stride\n",
    "    \n",
    "    min_y = y - stride\n",
    "    max_y = y + stride\n",
    "    image_glimpse = image[min_y:max_y, min_x:max_x, :]  # NOTE: row, column, RGB\n",
    "#     image_glimpse = image[min_y:max_y, min_x:max_x, 0]  # NOTE: row, column, RGB; everything is greyscale; flatten RGB layer\n",
    "    return imgToArr(image_glimpse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "This first model is the basic logistic regression, which we'll train using SGD. With this model, we can achieve 60% accuracy on the training set in 100 epochs.\n",
    " \n",
    "![alt text](https://www.tensorflow.org/versions/r0.9/images/softmax-regression-scalargraph.png \"Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Create a logistic regression model for brake classification with 28x28x3 image input.\"\"\"\n",
    "# Create placeholders for inputs that will be placed via batches\n",
    "image_input = tf.placeholder(tf.float32, [None, 28*28*3], name=\"image\")\n",
    "gaze_input = tf.placeholder(tf.float32, [None, 2], name=\"gaze\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 2], name=\"output\")\n",
    "\n",
    "image_weights = tf.Variable(tf.truncated_normal([28*28*3, 2], stddev=1), name=\"image_weights\")\n",
    "gaze_weights = tf.Variable(tf.truncated_normal([2, 2], stddev=1), name=\"gaze_weights\")\n",
    "\n",
    "image_bias = tf.Variable(tf.truncated_normal([2], stddev=1), name=\"image_bias\")\n",
    "gaze_bias = tf.Variable(tf.truncated_normal([2], stddev=1), name=\"gaze_bias\")\n",
    "\n",
    "image_logits = tf.matmul(image_input, image_weights) + image_bias\n",
    "gaze_logits = tf.matmul(gaze_input, gaze_weights) + gaze_bias\n",
    "\n",
    "logits = tf.mul(tf.add(image_logits, gaze_logits), 0.5)\n",
    "y = tf.nn.softmax(logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.reduce_sum(-y_*tf.log(tf.clip_by_value(y, 1e-10,1.0)),reduction_indices=[1]))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "# initialization of variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Define computations for accuracy calculation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCross-entropy: 11.621\tAccuracy: 0.494\n",
      "\tCross-entropy: 11.270\tAccuracy: 0.509\n",
      "\tCross-entropy: 10.414\tAccuracy: 0.544\n",
      "\tCross-entropy: 10.124\tAccuracy: 0.556\n",
      "\tCross-entropy: 10.054\tAccuracy: 0.560\n",
      "\tCross-entropy: 10.035\tAccuracy: 0.560\n",
      "\tCross-entropy: 10.022\tAccuracy: 0.561\n",
      "\tCross-entropy: 10.006\tAccuracy: 0.562\n",
      "\tCross-entropy: 9.958\tAccuracy: 0.564\n",
      "\tCross-entropy: 9.864\tAccuracy: 0.569\n",
      "\tCross-entropy: 9.816\tAccuracy: 0.571\n",
      "\tCross-entropy: 9.736\tAccuracy: 0.575\n",
      "\tCross-entropy: 9.671\tAccuracy: 0.578\n",
      "\tCross-entropy: 9.641\tAccuracy: 0.579\n",
      "\tCross-entropy: 9.566\tAccuracy: 0.583\n",
      "\tCross-entropy: 9.535\tAccuracy: 0.584\n",
      "\tCross-entropy: 9.510\tAccuracy: 0.585\n",
      "\tCross-entropy: 9.460\tAccuracy: 0.587\n",
      "\tCross-entropy: 9.440\tAccuracy: 0.588\n",
      "\tCross-entropy: 9.440\tAccuracy: 0.588\n",
      "\tCross-entropy: 9.432\tAccuracy: 0.588\n",
      "\tCross-entropy: 9.414\tAccuracy: 0.589\n",
      "\tCross-entropy: 9.373\tAccuracy: 0.591\n",
      "\tCross-entropy: 9.356\tAccuracy: 0.592\n",
      "\tCross-entropy: 9.338\tAccuracy: 0.593\n",
      "\tCross-entropy: 9.332\tAccuracy: 0.593\n",
      "\tCross-entropy: 9.320\tAccuracy: 0.594\n",
      "\tCross-entropy: 9.337\tAccuracy: 0.593\n",
      "\tCross-entropy: 9.382\tAccuracy: 0.591\n",
      "\tCross-entropy: 9.347\tAccuracy: 0.592\n",
      "\tCross-entropy: 9.292\tAccuracy: 0.595\n",
      "\tCross-entropy: 9.301\tAccuracy: 0.595\n",
      "\tCross-entropy: 9.271\tAccuracy: 0.596\n",
      "\tCross-entropy: 9.283\tAccuracy: 0.595\n",
      "\tCross-entropy: 9.280\tAccuracy: 0.595\n",
      "\tCross-entropy: 9.256\tAccuracy: 0.597\n",
      "\tCross-entropy: 9.266\tAccuracy: 0.596\n",
      "\tCross-entropy: 9.308\tAccuracy: 0.595\n",
      "\tCross-entropy: 9.254\tAccuracy: 0.597\n",
      "\tCross-entropy: 9.220\tAccuracy: 0.598\n",
      "\tCross-entropy: 9.202\tAccuracy: 0.599\n",
      "\tCross-entropy: 9.233\tAccuracy: 0.597\n",
      "\tCross-entropy: 9.204\tAccuracy: 0.599\n",
      "\tCross-entropy: 9.213\tAccuracy: 0.599\n",
      "\tCross-entropy: 9.190\tAccuracy: 0.599\n",
      "\tCross-entropy: 9.174\tAccuracy: 0.600\n",
      "\tCross-entropy: 9.175\tAccuracy: 0.600\n",
      "\tCross-entropy: 9.168\tAccuracy: 0.600\n",
      "\tCross-entropy: 9.163\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.161\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.156\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.140\tAccuracy: 0.602\n",
      "\tCross-entropy: 9.153\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.154\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.239\tAccuracy: 0.597\n",
      "\tCross-entropy: 9.253\tAccuracy: 0.597\n",
      "\tCross-entropy: 9.162\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.158\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.197\tAccuracy: 0.599\n",
      "\tCross-entropy: 9.134\tAccuracy: 0.602\n",
      "\tCross-entropy: 9.161\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.280\tAccuracy: 0.595\n",
      "\tCross-entropy: 9.156\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.102\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.104\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.100\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.095\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.092\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.080\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.118\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.135\tAccuracy: 0.602\n",
      "\tCross-entropy: 9.130\tAccuracy: 0.602\n",
      "\tCross-entropy: 9.089\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.095\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.069\tAccuracy: 0.605\n",
      "\tCross-entropy: 9.101\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.122\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.127\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.111\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.089\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.046\tAccuracy: 0.606\n",
      "\tCross-entropy: 9.095\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.038\tAccuracy: 0.606\n",
      "\tCross-entropy: 9.053\tAccuracy: 0.606\n",
      "\tCross-entropy: 9.062\tAccuracy: 0.605\n",
      "\tCross-entropy: 9.038\tAccuracy: 0.606\n",
      "\tCross-entropy: 9.029\tAccuracy: 0.607\n",
      "\tCross-entropy: 9.027\tAccuracy: 0.607\n",
      "\tCross-entropy: 9.027\tAccuracy: 0.607\n",
      "\tCross-entropy: 9.064\tAccuracy: 0.605\n",
      "\tCross-entropy: 9.050\tAccuracy: 0.606\n",
      "\tCross-entropy: 9.026\tAccuracy: 0.607\n",
      "\tCross-entropy: 9.008\tAccuracy: 0.608\n",
      "\tCross-entropy: 9.015\tAccuracy: 0.607\n",
      "\tCross-entropy: 9.040\tAccuracy: 0.606\n",
      "\tCross-entropy: 9.013\tAccuracy: 0.607\n",
      "\tCross-entropy: 9.009\tAccuracy: 0.608\n",
      "\tCross-entropy: 8.992\tAccuracy: 0.608\n",
      "\tCross-entropy: 8.988\tAccuracy: 0.609\n",
      "\tCross-entropy: 8.980\tAccuracy: 0.609\n"
     ]
    }
   ],
   "source": [
    "input_glimpse_flat = input_glimpses.reshape(-1, 28*28*3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    indices = range(len(input_glimpses))\n",
    "    for epoch in range(100):\n",
    "        batches = minibatch(indices, 10000, len(indices))\n",
    "\n",
    "        for index_batch in batches:\n",
    "            glimpses = input_glimpses[index_batch]\n",
    "            glimpses = glimpses.reshape(-1, 28*28*3)\n",
    "            gazes = input_gazes[index_batch]\n",
    "            output = outputs[index_batch]\n",
    "#             print(\"Nonbraking: {:.0f}\\tBraking: {:.0f}\".format(output[:, 0].sum(), output[:, 1].sum()))\n",
    "            sess.run(optimizer, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "        ce = sess.run(cross_entropy, feed_dict={image_input: input_glimpse_flat, gaze_input: input_gazes, y_: outputs})\n",
    "        acc = sess.run(accuracy, feed_dict={image_input: input_glimpse_flat, gaze_input: input_gazes, y_: outputs})\n",
    "#         pred = sess.run(y, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "#         num_pred_nonbrake = pred[:, 0].sum()\n",
    "#         num_pred_brake = pred[:, 1].sum()\n",
    "#         print(\"\\tNon-brake: {:.0f}\\tBrake: {:.0f}\".format(num_pred_nonbrake, num_pred_brake))\n",
    "        print(\"\\tCross-entropy: {:.3f}\\tAccuracy: {:.3f}\".format(ce, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feedforward Neural Net\n",
    "Our next model will be a feedforward neural network with a ReLu activation function with a 1024 neuron hidden layer. We'll train it using the same methodology as the logistic regression model (SGD and cross-entropy).\n",
    "\n",
    "![alt text](http://cs231n.github.io/assets/nn1/neural_net.jpeg \"Neural Network\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Create neural network for brake classification with 28x28x3 image input.\"\"\"\n",
    "# Create placeholders for inputs that will be placed via batches\n",
    "image_input = tf.placeholder(tf.float32, [None, 28*28*3], name=\"image\")\n",
    "gaze_input = tf.placeholder(tf.float32, [None, 2], name=\"gaze\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 2], name=\"output\")\n",
    "\n",
    "image_weights = tf.Variable(tf.truncated_normal([28*28*3, 1024], stddev=1), name=\"image_weights\")\n",
    "image_hidden_weights = tf.Variable(tf.truncated_normal([1024, 2], stddev=1), name=\"image_hidden_weights\")\n",
    "\n",
    "gaze_weights = tf.Variable(tf.truncated_normal([2, 1024], stddev=1), name=\"gaze_weights\")\n",
    "gaze_hidden_weights = tf.Variable(tf.truncated_normal([1024, 2], stddev=1), name=\"gaze_hidden_weights\")\n",
    "\n",
    "image_bias = tf.Variable(tf.truncated_normal([1024], stddev=1), name=\"image_bias\")\n",
    "image_hidden_bias = tf.Variable(tf.truncated_normal([2], stddev=1), name=\"image_hidden_bias\")\n",
    "\n",
    "gaze_bias = tf.Variable(tf.truncated_normal([1024], stddev=1), name=\"gaze_bias\")\n",
    "gaze_hidden_bias = tf.Variable(tf.truncated_normal([2], stddev=1), name=\"gaze_hidden_bias\")\n",
    "\n",
    "image_input_layer = tf.matmul(image_input, image_weights) + image_bias\n",
    "image_hidden_layer = tf.matmul(tf.nn.relu(image_input_layer), image_hidden_weights) + image_hidden_bias\n",
    "\n",
    "gaze_input_layer = tf.matmul(gaze_input, gaze_weights) + gaze_bias\n",
    "gaze_hidden_layer = tf.matmul(tf.nn.relu(gaze_input_layer), gaze_hidden_weights) + gaze_hidden_bias\n",
    "\n",
    "logits = tf.mul(tf.add(image_hidden_layer, gaze_hidden_layer), 0.5)\n",
    "y = tf.nn.softmax(logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.reduce_sum(-y_*tf.log(tf.clip_by_value(y, 1e-10,1.0)),reduction_indices=[1]))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "# initialization of variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Define computations for accuracy calculation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCross-entropy: 11.378\tAccuracy: 0.506\n",
      "\tCross-entropy: 11.378\tAccuracy: 0.506\n",
      "\tCross-entropy: 11.378\tAccuracy: 0.506\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9c359468e699>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mglimpses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaze_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgazes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_glimpse_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaze_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_gazes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_glimpse_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaze_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_gazes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#         pred = sess.run(y, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#         num_pred_nonbrake = pred[:, 0].sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_glimpse_flat = input_glimpses.reshape(-1, 28*28*3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    indices = range(len(input_glimpses))\n",
    "    for epoch in range(100):\n",
    "        batches = minibatch(indices, 1000, len(indices))\n",
    "\n",
    "        for index_batch in batches:\n",
    "            glimpses = input_glimpses[index_batch]\n",
    "            glimpses = glimpses.reshape(-1, 28*28*3)\n",
    "            gazes = input_gazes[index_batch]\n",
    "            output = outputs[index_batch]\n",
    "#             print(\"Nonbraking: {:.0f}\\tBraking: {:.0f}\".format(output[:, 0].sum(), output[:, 1].sum()))\n",
    "            sess.run(optimizer, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "        ce = sess.run(cross_entropy, feed_dict={image_input: input_glimpse_flat, gaze_input: input_gazes, y_: outputs})\n",
    "        acc = sess.run(accuracy, feed_dict={image_input: input_glimpse_flat, gaze_input: input_gazes, y_: outputs})\n",
    "#         pred = sess.run(y, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "#         num_pred_nonbrake = pred[:, 0].sum()\n",
    "#         num_pred_brake = pred[:, 1].sum()\n",
    "#         print(\"\\tNon-brake: {:.0f}\\tBrake: {:.0f}\".format(num_pred_nonbrake, num_pred_brake))\n",
    "        print(\"\\tCross-entropy: {:.3f}\\tAccuracy: {:.3f}\".format(ce, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "The feedforward network's results were promising! With just one hidden layer, we brought the initial training cross-entropy down from 11 to 10 (still bad, but it's getting better). Now, we'll try a wide-and-deep network where the deep part is a convnet on the image and the wide portion processes the driver gaze's coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define some helper methods that will abstract variable initialization and layer definitions\n",
    "def weight_variable(shape, wd=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)  # Store losses in a collection\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape, wd=None):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)  # Store losses in a collection\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, name='conv'):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name=name)\n",
    "\n",
    "def max_pool_2x2(x, name='max_pool_2x2'):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define convolutional computation graph\n",
    "\n",
    "# Placeholders for parameters: image, gaze (x, y), output, dropout probability\n",
    "image_input = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"image\")\n",
    "gaze_input = tf.placeholder(tf.float32, [None, 2], name=\"gaze\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 2], name=\"output\")\n",
    "keep_prob = 0.5\n",
    "\n",
    "\n",
    "# Convolutional net for image processing\n",
    "\n",
    "# First layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])  # 5x5x1 filter with 32 features\n",
    "b_conv1 = bias_variable([32])             # Bias for each filter\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(image_input, W_conv1) + b_conv1, name='conv1')\n",
    "h_pool1 = max_pool_2x2(h_conv1, name='pool1')\n",
    "\n",
    "# Second layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, name='conv2')\n",
    "h_pool2 = max_pool_2x2(h_conv2, name='pool2')\n",
    "\n",
    "# Fully-connected layer hidden layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, 'hidden1')\n",
    "\n",
    "# Add dropout for fully-connected hidden layer\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Final logits\n",
    "W_fc2 = weight_variable([1024, 2])\n",
    "b_fc2 = bias_variable([2])\n",
    "\n",
    "image_logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "# Logistic regression for human gaze\n",
    "W_g = weight_variable([2, 2])\n",
    "b_g = bias_variable([2])\n",
    "\n",
    "gaze_logits = tf.matmul(gaze_input, W_g) + b_g\n",
    "\n",
    "\n",
    "# Combine logistic and convnet\n",
    "logits = tf.mul(tf.add(image_logits, gaze_logits), 0.5)\n",
    "y = tf.nn.softmax(logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.reduce_sum(-y_*tf.log(tf.clip_by_value(y, 1e-10,1.0)),reduction_indices=[1]))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "# initialization of variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Define computations for accuracy calculation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonbraking: 4908\tBraking: 5092\n",
      "\tNon-brake: 2006\tBrake: 7994\n",
      "\tCross-entropy: 9.360\tAccuracy: 0.595\n",
      "Nonbraking: 4923\tBraking: 5077\n",
      "\tNon-brake: 8506\tBrake: 1494\n",
      "\tCross-entropy: 10.175\tAccuracy: 0.554\n",
      "Nonbraking: 4915\tBraking: 5084\n",
      "\tNon-brake: 5573\tBrake: 4427\n",
      "\tCross-entropy: 8.340\tAccuracy: 0.633\n",
      "Nonbraking: 4913\tBraking: 5084\n",
      "\tNon-brake: 630\tBrake: 9370\n",
      "\tCross-entropy: 10.366\tAccuracy: 0.549\n",
      "Nonbraking: 5010\tBraking: 4988\n",
      "\tNon-brake: 628\tBrake: 9372\n",
      "\tCross-entropy: 10.506\tAccuracy: 0.541\n",
      "Nonbraking: 5000\tBraking: 5000\n",
      "\tNon-brake: 1757\tBrake: 8243\n",
      "\tCross-entropy: 9.194\tAccuracy: 0.603\n",
      "Nonbraking: 4925\tBraking: 5075\n",
      "\tNon-brake: 4335\tBrake: 5665\n",
      "\tCross-entropy: 7.824\tAccuracy: 0.657\n",
      "Nonbraking: 4935\tBraking: 5063\n",
      "\tNon-brake: 7501\tBrake: 2499\n",
      "\tCross-entropy: 9.384\tAccuracy: 0.588\n",
      "Nonbraking: 4910\tBraking: 5089\n",
      "\tNon-brake: 8317\tBrake: 1683\n",
      "\tCross-entropy: 10.023\tAccuracy: 0.563\n",
      "Nonbraking: 4928\tBraking: 5072\n",
      "\tNon-brake: 8051\tBrake: 1949\n",
      "\tCross-entropy: 9.580\tAccuracy: 0.580\n",
      "Nonbraking: 5005\tBraking: 4995\n",
      "\tNon-brake: 7287\tBrake: 2713\n",
      "\tCross-entropy: 9.085\tAccuracy: 0.603\n",
      "Nonbraking: 4872\tBraking: 5125\n",
      "\tNon-brake: 5838\tBrake: 4162\n",
      "\tCross-entropy: 8.040\tAccuracy: 0.644\n",
      "Nonbraking: 4942\tBraking: 5057\n",
      "\tNon-brake: 4143\tBrake: 5857\n",
      "\tCross-entropy: 7.338\tAccuracy: 0.681\n",
      "Nonbraking: 4965\tBraking: 5033\n",
      "\tNon-brake: 2784\tBrake: 7216\n",
      "\tCross-entropy: 7.649\tAccuracy: 0.665\n",
      "Nonbraking: 4888\tBraking: 5111\n",
      "\tNon-brake: 1530\tBrake: 8470\n",
      "\tCross-entropy: 8.915\tAccuracy: 0.611\n",
      "Nonbraking: 5019\tBraking: 4981\n",
      "\tNon-brake: 2088\tBrake: 7912\n",
      "\tCross-entropy: 8.314\tAccuracy: 0.635\n",
      "Nonbraking: 4954\tBraking: 5046\n",
      "\tNon-brake: 3266\tBrake: 6734\n",
      "\tCross-entropy: 7.363\tAccuracy: 0.681\n",
      "Nonbraking: 4857\tBraking: 5142\n",
      "\tNon-brake: 4408\tBrake: 5592\n",
      "\tCross-entropy: 7.274\tAccuracy: 0.685\n",
      "Nonbraking: 4936\tBraking: 5062\n",
      "\tNon-brake: 4839\tBrake: 5161\n",
      "\tCross-entropy: 7.257\tAccuracy: 0.686\n",
      "Nonbraking: 4955\tBraking: 5043\n",
      "\tNon-brake: 4207\tBrake: 5793\n",
      "\tCross-entropy: 6.968\tAccuracy: 0.696\n",
      "Nonbraking: 4968\tBraking: 5032\n",
      "\tNon-brake: 3695\tBrake: 6305\n",
      "\tCross-entropy: 6.892\tAccuracy: 0.701\n",
      "Nonbraking: 4924\tBraking: 5075\n",
      "\tNon-brake: 3146\tBrake: 6854\n",
      "\tCross-entropy: 7.050\tAccuracy: 0.686\n",
      "Nonbraking: 4959\tBraking: 5041\n",
      "\tNon-brake: 3019\tBrake: 6981\n",
      "\tCross-entropy: 7.247\tAccuracy: 0.683\n",
      "Nonbraking: 4976\tBraking: 5022\n",
      "\tNon-brake: 3528\tBrake: 6472\n",
      "\tCross-entropy: 7.004\tAccuracy: 0.697\n",
      "Nonbraking: 4842\tBraking: 5157\n",
      "\tNon-brake: 3792\tBrake: 6208\n",
      "\tCross-entropy: 6.770\tAccuracy: 0.704\n",
      "Nonbraking: 4850\tBraking: 5150\n",
      "\tNon-brake: 3771\tBrake: 6229\n",
      "\tCross-entropy: 6.711\tAccuracy: 0.707\n",
      "Nonbraking: 5025\tBraking: 4972\n",
      "\tNon-brake: 3978\tBrake: 6022\n",
      "\tCross-entropy: 6.725\tAccuracy: 0.700\n",
      "Nonbraking: 4991\tBraking: 5008\n",
      "\tNon-brake: 4111\tBrake: 5889\n",
      "\tCross-entropy: 6.670\tAccuracy: 0.709\n",
      "Nonbraking: 4931\tBraking: 5068\n",
      "\tNon-brake: 4120\tBrake: 5880\n",
      "\tCross-entropy: 6.645\tAccuracy: 0.709\n",
      "Nonbraking: 4990\tBraking: 5009\n",
      "\tNon-brake: 4254\tBrake: 5746\n",
      "\tCross-entropy: 6.649\tAccuracy: 0.709\n",
      "Nonbraking: 4906\tBraking: 5094\n",
      "\tNon-brake: 4133\tBrake: 5867\n",
      "\tCross-entropy: 6.575\tAccuracy: 0.716\n",
      "Nonbraking: 4994\tBraking: 5005\n",
      "\tNon-brake: 4429\tBrake: 5571\n",
      "\tCross-entropy: 6.423\tAccuracy: 0.721\n",
      "Nonbraking: 4915\tBraking: 5085\n",
      "\tNon-brake: 4458\tBrake: 5542\n",
      "\tCross-entropy: 6.446\tAccuracy: 0.722\n",
      "Nonbraking: 4867\tBraking: 5133\n",
      "\tNon-brake: 4744\tBrake: 5256\n",
      "\tCross-entropy: 6.350\tAccuracy: 0.725\n",
      "Nonbraking: 4985\tBraking: 5011\n",
      "\tNon-brake: 4703\tBrake: 5297\n",
      "\tCross-entropy: 6.082\tAccuracy: 0.731\n",
      "Nonbraking: 4905\tBraking: 5095\n",
      "\tNon-brake: 4301\tBrake: 5699\n",
      "\tCross-entropy: 6.196\tAccuracy: 0.728\n",
      "Nonbraking: 5004\tBraking: 4994\n",
      "\tNon-brake: 4159\tBrake: 5841\n",
      "\tCross-entropy: 6.401\tAccuracy: 0.723\n",
      "Nonbraking: 4867\tBraking: 5132\n",
      "\tNon-brake: 3744\tBrake: 6256\n",
      "\tCross-entropy: 6.529\tAccuracy: 0.714\n",
      "Nonbraking: 4988\tBraking: 5012\n",
      "\tNon-brake: 4555\tBrake: 5445\n",
      "\tCross-entropy: 6.098\tAccuracy: 0.735\n",
      "Nonbraking: 4998\tBraking: 5001\n",
      "\tNon-brake: 5936\tBrake: 4064\n",
      "\tCross-entropy: 6.351\tAccuracy: 0.725\n",
      "Nonbraking: 4948\tBraking: 5052\n",
      "\tNon-brake: 6267\tBrake: 3733\n",
      "\tCross-entropy: 6.707\tAccuracy: 0.706\n",
      "Nonbraking: 4935\tBraking: 5063\n",
      "\tNon-brake: 5987\tBrake: 4013\n",
      "\tCross-entropy: 6.397\tAccuracy: 0.720\n",
      "Nonbraking: 4923\tBraking: 5076\n",
      "\tNon-brake: 4838\tBrake: 5162\n",
      "\tCross-entropy: 5.932\tAccuracy: 0.739\n",
      "Nonbraking: 4919\tBraking: 5080\n",
      "\tNon-brake: 3439\tBrake: 6561\n",
      "\tCross-entropy: 6.698\tAccuracy: 0.712\n",
      "Nonbraking: 4972\tBraking: 5027\n",
      "\tNon-brake: 3151\tBrake: 6849\n",
      "\tCross-entropy: 6.973\tAccuracy: 0.699\n",
      "Nonbraking: 4921\tBraking: 5079\n",
      "\tNon-brake: 3500\tBrake: 6500\n",
      "\tCross-entropy: 6.462\tAccuracy: 0.717\n",
      "Nonbraking: 4964\tBraking: 5036\n",
      "\tNon-brake: 4530\tBrake: 5470\n",
      "\tCross-entropy: 6.010\tAccuracy: 0.735\n",
      "Nonbraking: 4947\tBraking: 5050\n",
      "\tNon-brake: 5474\tBrake: 4526\n",
      "\tCross-entropy: 5.918\tAccuracy: 0.738\n",
      "Nonbraking: 4935\tBraking: 5063\n",
      "\tNon-brake: 5871\tBrake: 4129\n",
      "\tCross-entropy: 6.142\tAccuracy: 0.729\n",
      "Nonbraking: 4971\tBraking: 5027\n",
      "\tNon-brake: 5990\tBrake: 4010\n",
      "\tCross-entropy: 6.540\tAccuracy: 0.714\n",
      "Nonbraking: 4964\tBraking: 5036\n",
      "\tNon-brake: 5046\tBrake: 4954\n",
      "\tCross-entropy: 5.776\tAccuracy: 0.744\n",
      "Nonbraking: 4934\tBraking: 5065\n",
      "\tNon-brake: 3656\tBrake: 6344\n",
      "\tCross-entropy: 6.230\tAccuracy: 0.730\n",
      "Nonbraking: 4954\tBraking: 5044\n",
      "\tNon-brake: 3962\tBrake: 6038\n",
      "\tCross-entropy: 6.135\tAccuracy: 0.731\n",
      "Nonbraking: 4988\tBraking: 5012\n",
      "\tNon-brake: 4494\tBrake: 5506\n",
      "\tCross-entropy: 5.720\tAccuracy: 0.748\n",
      "Nonbraking: 4848\tBraking: 5151\n",
      "\tNon-brake: 5298\tBrake: 4702\n",
      "\tCross-entropy: 5.818\tAccuracy: 0.745\n",
      "Nonbraking: 4935\tBraking: 5065\n",
      "\tNon-brake: 6402\tBrake: 3598\n",
      "\tCross-entropy: 6.560\tAccuracy: 0.713\n",
      "Nonbraking: 5002\tBraking: 4997\n",
      "\tNon-brake: 6602\tBrake: 3398\n",
      "\tCross-entropy: 6.856\tAccuracy: 0.698\n",
      "Nonbraking: 5007\tBraking: 4991\n",
      "\tNon-brake: 6045\tBrake: 3955\n",
      "\tCross-entropy: 6.069\tAccuracy: 0.734\n",
      "Nonbraking: 4983\tBraking: 5017\n",
      "\tNon-brake: 5270\tBrake: 4730\n",
      "\tCross-entropy: 5.890\tAccuracy: 0.742\n",
      "Nonbraking: 4988\tBraking: 5010\n",
      "\tNon-brake: 4437\tBrake: 5563\n",
      "\tCross-entropy: 5.788\tAccuracy: 0.745\n",
      "Nonbraking: 4918\tBraking: 5082\n",
      "\tNon-brake: 4136\tBrake: 5864\n",
      "\tCross-entropy: 5.871\tAccuracy: 0.742\n",
      "Nonbraking: 4839\tBraking: 5160\n",
      "\tNon-brake: 4055\tBrake: 5945\n",
      "\tCross-entropy: 6.099\tAccuracy: 0.736\n",
      "Nonbraking: 4881\tBraking: 5118\n",
      "\tNon-brake: 4177\tBrake: 5823\n",
      "\tCross-entropy: 5.716\tAccuracy: 0.749\n",
      "Nonbraking: 4911\tBraking: 5088\n",
      "\tNon-brake: 4614\tBrake: 5386\n",
      "\tCross-entropy: 5.671\tAccuracy: 0.753\n",
      "Nonbraking: 4903\tBraking: 5096\n",
      "\tNon-brake: 4808\tBrake: 5192\n",
      "\tCross-entropy: 5.612\tAccuracy: 0.754\n",
      "Nonbraking: 4884\tBraking: 5115\n",
      "\tNon-brake: 5066\tBrake: 4934\n",
      "\tCross-entropy: 5.483\tAccuracy: 0.757\n",
      "Nonbraking: 4942\tBraking: 5057\n",
      "\tNon-brake: 5219\tBrake: 4781\n",
      "\tCross-entropy: 5.821\tAccuracy: 0.739\n",
      "Nonbraking: 4995\tBraking: 5004\n",
      "\tNon-brake: 4563\tBrake: 5437\n",
      "\tCross-entropy: 5.718\tAccuracy: 0.749\n",
      "Nonbraking: 4978\tBraking: 5019\n",
      "\tNon-brake: 3924\tBrake: 6076\n",
      "\tCross-entropy: 6.062\tAccuracy: 0.730\n",
      "Nonbraking: 4910\tBraking: 5090\n",
      "\tNon-brake: 3427\tBrake: 6573\n",
      "\tCross-entropy: 6.574\tAccuracy: 0.711\n",
      "Nonbraking: 4988\tBraking: 5011\n",
      "\tNon-brake: 3663\tBrake: 6337\n",
      "\tCross-entropy: 6.320\tAccuracy: 0.722\n",
      "Nonbraking: 4929\tBraking: 5071\n",
      "\tNon-brake: 4299\tBrake: 5701\n",
      "\tCross-entropy: 5.775\tAccuracy: 0.746\n",
      "Nonbraking: 4965\tBraking: 5033\n",
      "\tNon-brake: 4824\tBrake: 5176\n",
      "\tCross-entropy: 5.586\tAccuracy: 0.753\n",
      "Nonbraking: 4887\tBraking: 5113\n",
      "\tNon-brake: 4932\tBrake: 5068\n",
      "\tCross-entropy: 5.641\tAccuracy: 0.753\n",
      "Nonbraking: 4974\tBraking: 5026\n",
      "\tNon-brake: 5338\tBrake: 4662\n",
      "\tCross-entropy: 5.775\tAccuracy: 0.750\n",
      "Nonbraking: 4966\tBraking: 5034\n",
      "\tNon-brake: 5219\tBrake: 4781\n",
      "\tCross-entropy: 5.595\tAccuracy: 0.756\n",
      "Nonbraking: 4906\tBraking: 5093\n",
      "\tNon-brake: 4827\tBrake: 5173\n",
      "\tCross-entropy: 5.501\tAccuracy: 0.760\n",
      "Nonbraking: 4948\tBraking: 5050\n",
      "\tNon-brake: 4322\tBrake: 5678\n",
      "\tCross-entropy: 5.704\tAccuracy: 0.750\n",
      "Nonbraking: 4933\tBraking: 5066\n",
      "\tNon-brake: 3826\tBrake: 6174\n",
      "\tCross-entropy: 5.957\tAccuracy: 0.738\n",
      "Nonbraking: 4950\tBraking: 5048\n",
      "\tNon-brake: 3667\tBrake: 6333\n",
      "\tCross-entropy: 6.202\tAccuracy: 0.730\n",
      "Nonbraking: 4894\tBraking: 5106\n",
      "\tNon-brake: 3527\tBrake: 6473\n",
      "\tCross-entropy: 6.087\tAccuracy: 0.727\n",
      "Nonbraking: 5007\tBraking: 4992\n",
      "\tNon-brake: 4557\tBrake: 5443\n",
      "\tCross-entropy: 5.453\tAccuracy: 0.759\n",
      "Nonbraking: 4947\tBraking: 5053\n",
      "\tNon-brake: 5859\tBrake: 4141\n",
      "\tCross-entropy: 6.010\tAccuracy: 0.735\n",
      "Nonbraking: 4955\tBraking: 5044\n",
      "\tNon-brake: 7094\tBrake: 2906\n",
      "\tCross-entropy: 7.183\tAccuracy: 0.687\n",
      "Nonbraking: 4923\tBraking: 5076\n",
      "\tNon-brake: 7807\tBrake: 2193\n",
      "\tCross-entropy: 8.004\tAccuracy: 0.648\n",
      "Nonbraking: 4898\tBraking: 5102\n",
      "\tNon-brake: 8030\tBrake: 1970\n",
      "\tCross-entropy: 8.524\tAccuracy: 0.630\n",
      "Nonbraking: 4896\tBraking: 5102\n",
      "\tNon-brake: 7856\tBrake: 2144\n",
      "\tCross-entropy: 8.226\tAccuracy: 0.641\n",
      "Nonbraking: 5009\tBraking: 4988\n",
      "\tNon-brake: 7321\tBrake: 2679\n",
      "\tCross-entropy: 7.429\tAccuracy: 0.674\n",
      "Nonbraking: 4884\tBraking: 5115\n",
      "\tNon-brake: 6040\tBrake: 3960\n",
      "\tCross-entropy: 6.134\tAccuracy: 0.732\n",
      "Nonbraking: 5009\tBraking: 4990\n",
      "\tNon-brake: 4453\tBrake: 5547\n",
      "\tCross-entropy: 5.669\tAccuracy: 0.755\n",
      "Nonbraking: 4973\tBraking: 5026\n",
      "\tNon-brake: 3139\tBrake: 6861\n",
      "\tCross-entropy: 6.628\tAccuracy: 0.714\n",
      "Nonbraking: 4959\tBraking: 5040\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-1f0a85fd814f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nonbraking: {:.0f}\\tBraking: {:.0f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mglimpses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaze_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgazes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mglimpses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaze_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgazes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mglimpses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaze_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgazes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    indices = range(len(input_glimpses))\n",
    "    for epoch in range(100):\n",
    "        batches = minibatch(indices, 10000, len(indices))\n",
    "\n",
    "        for index_batch in batches:\n",
    "            glimpses = input_glimpses[index_batch, :, :, :1]\n",
    "            gazes = input_gazes[index_batch]\n",
    "            output = outputs[index_batch]\n",
    "            print(\"Nonbraking: {:.0f}\\tBraking: {:.0f}\".format(output[:, 0].sum(), output[:, 1].sum()))\n",
    "            sess.run(optimizer, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "            ce = sess.run(cross_entropy, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "            acc = sess.run(accuracy, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "            pred = sess.run(y, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "            num_pred_nonbrake = pred[:, 0].sum()\n",
    "            num_pred_brake = pred[:, 1].sum()\n",
    "            print(\"\\tNon-brake: {:.0f}\\tBrake: {:.0f}\".format(num_pred_nonbrake, num_pred_brake))\n",
    "            print(\"\\tCross-entropy: {:.3f}\\tAccuracy: {:.3f}\".format(ce, acc))\n",
    "#                 if index == 67:\n",
    "#                     fig = pylab.figure()\n",
    "#                     pylab.imshow(this_glimpse)\n",
    "#                     pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
