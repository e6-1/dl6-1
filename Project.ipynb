{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autonomous Braking\n",
    "\n",
    "## Ethan Petersen, Josh Laesch, Ben Wong\n",
    "\n",
    "### The Dream Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as pylab\n",
    "import imageio\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imageio.core.util import asarray as imgToArr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "We perform some preprocessing on the data before feeding into the models. Namely, we will split the data into training, validation, and tests sets that will have equivalent amounts of braking and nonbraking frames. Note: frames 80,000 to 100,000 correspond to congested city center traffic following a large truck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# videoFile = './data/driving.avi'\n",
    "# vid = imageio.get_reader(videoFile,  'ffmpeg')\n",
    "\n",
    "# Columns: Frame, Brake, GazeX, GazeY\n",
    "dataFile = './data/cleaned_data.csv'\n",
    "df = pd.read_csv(dataFile, delimiter='\\t')\n",
    "\n",
    "brake = df[df['Brake'] > 0]\n",
    "nonbrake = df[df['Brake'] == 0]\n",
    "nonbrake = nonbrake[:len(brake)]  # Braking is far fewer than nonbraking, so trim down\n",
    "df = pd.concat([brake, nonbrake])\n",
    "df = df.drop(df[df['GazeX'] < 0].index)\n",
    "df = df.drop(df[df['GazeY'] < 0].index)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)  # Resets the index to the usual 0, 1, 2, ...\n",
    "\n",
    "# One-hot encode brakes\n",
    "outputs = OneHotEncoder(sparse=False).fit_transform(df['Brake'].reshape(-1,1))  # column 0: no brake, column 1: brake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper methods\n",
    "We define some helper functions: `minibatch` will get batches of data for training via SGD, `get_glimpses` processes a list of images and cuts out small glimpses based on a list of center coordinates, and `get_glimpse` performs the cutting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sub_seq(seq, start, end):\n",
    "    \"\"\"Get the sub sequence starting at the start index and ending at the end index.\"\"\"\n",
    "    arr = seq[max([0, start]):end]\n",
    "    if start < 0:\n",
    "        arr = np.append(np.zeros((abs(start),2)), arr, axis=0)\n",
    "    for i in range(len(arr)):\n",
    "        if np.sum(arr[i]) == 0:\n",
    "            arr[i] = [1, 0]\n",
    "    return arr\n",
    "\n",
    "def minibatch(data, batch_size, data_size):\n",
    "    \"\"\"Generates a minibatch from the given data and parameters.\"\"\"\n",
    "    randomized = np.random.permutation(data)\n",
    "    batches = []\n",
    "    num_batches = 0\n",
    "    while num_batches * batch_size < data_size:\n",
    "        new_batch = randomized[num_batches * batch_size:(num_batches + 1) * batch_size]\n",
    "        batches.append(new_batch)\n",
    "        num_batches += 1\n",
    "    return batches\n",
    "\n",
    "def get_glimpses(images, coords):\n",
    "    \"\"\"Gets a batch of glimpses.\"\"\"\n",
    "    arr = []\n",
    "    for img, coord in zip(images, coords):\n",
    "        arr.append(get_glimpse(img, coord[0], coord[1]))\n",
    "    return np.array(arr)\n",
    "\n",
    "def get_glimpse(image, x, y, stride=14):\n",
    "    \"\"\"Returns a subsection (glimpse) of the image centered on the given point.\"\"\"\n",
    "    x = int(x)  # Force to int\n",
    "    y = int(y)  # Force to int\n",
    "    min_x = x - stride\n",
    "    max_x = x + stride\n",
    "    \n",
    "    min_y = y - stride\n",
    "    max_y = y + stride\n",
    "    image_glimpse = image[min_y:max_y, min_x:max_x, :]  # NOTE: row, column, RGB\n",
    "#     image_glimpse = image[min_y:max_y, min_x:max_x, 0]  # NOTE: row, column, RGB; everything is greyscale; flatten RGB layer\n",
    "    return imgToArr(image_glimpse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In training, we sometimes pull straight from the video and specify a stride length\n",
    "# but we have also stored 28x28x3 glimpses for each frame\n",
    "input_glimpses = np.zeros((80000, 28, 28, 3))\n",
    "input_gazes = np.zeros((80000, 2))\n",
    "outputs = np.zeros((80000, 2))\n",
    "for batch in range(1, 9):\n",
    "    file_name = \"data/glimpse_batchc_{0}.npz\".format(batch)\n",
    "    array = np.load(file_name)\n",
    "    input_glimpses[(batch - 1) * 10000: batch * 10000] = array['frames']\n",
    "    input_gazes[(batch - 1) * 10000: batch * 10000] = array['gazes']\n",
    "    outputs[(batch - 1) * 10000: batch * 10000] = array['braking']\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    if np.sum(outputs[i]) == 0:\n",
    "        outputs[i] = [1, 0]\n",
    "\n",
    "sequences = np.array([get_sub_seq(outputs, i-3, i) for i in range(len(outputs))])\n",
    "sequences = sequences.reshape(-1, 3*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_inds = minibatch(range(len(input_glimpses)), 10000, len(input_glimpses))[0]\n",
    "training_inds = [i for i in range(len(input_glimpses)) if i not in test_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "This first model is the basic logistic regression, which we'll train using SGD. With this model, we can achieve 60% accuracy on the training set in 100 epochs.\n",
    " \n",
    "![alt text](https://www.tensorflow.org/versions/r0.9/images/softmax-regression-scalargraph.png \"Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Create a logistic regression model for brake classification with 28x28x3 image input.\"\"\"\n",
    "# Create placeholders for inputs that will be placed via batches\n",
    "image_input = tf.placeholder(tf.float32, [None, 28*28*3], name=\"image\")\n",
    "gaze_input = tf.placeholder(tf.float32, [None, 2], name=\"gaze\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 2], name=\"output\")\n",
    "\n",
    "image_weights = tf.Variable(tf.truncated_normal([28*28*3, 2], stddev=1), name=\"image_weights\")\n",
    "gaze_weights = tf.Variable(tf.truncated_normal([2, 2], stddev=1), name=\"gaze_weights\")\n",
    "\n",
    "image_bias = tf.Variable(tf.truncated_normal([2], stddev=1), name=\"image_bias\")\n",
    "gaze_bias = tf.Variable(tf.truncated_normal([2], stddev=1), name=\"gaze_bias\")\n",
    "\n",
    "image_logits = tf.matmul(image_input, image_weights) + image_bias\n",
    "gaze_logits = tf.matmul(gaze_input, gaze_weights) + gaze_bias\n",
    "\n",
    "logits = tf.mul(tf.add(image_logits, gaze_logits), 0.5)\n",
    "y = tf.nn.softmax(logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.reduce_sum(-y_*tf.log(tf.clip_by_value(y, 1e-10,1.0)),reduction_indices=[1]))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "# initialization of variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Define computations for accuracy calculation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCross-entropy: 11.522\tAccuracy: 0.498\n",
      "\tCross-entropy: 10.991\tAccuracy: 0.520\n",
      "\tCross-entropy: 10.120\tAccuracy: 0.557\n",
      "\tCross-entropy: 10.072\tAccuracy: 0.559\n",
      "\tCross-entropy: 10.168\tAccuracy: 0.555\n",
      "\tCross-entropy: 10.046\tAccuracy: 0.560\n",
      "\tCross-entropy: 9.952\tAccuracy: 0.564\n",
      "\tCross-entropy: 9.952\tAccuracy: 0.564\n",
      "\tCross-entropy: 9.949\tAccuracy: 0.565\n",
      "\tCross-entropy: 9.901\tAccuracy: 0.567\n",
      "\tCross-entropy: 9.900\tAccuracy: 0.567\n",
      "\tCross-entropy: 9.863\tAccuracy: 0.568\n",
      "\tCross-entropy: 9.837\tAccuracy: 0.569\n",
      "\tCross-entropy: 9.835\tAccuracy: 0.570\n",
      "\tCross-entropy: 9.842\tAccuracy: 0.569\n",
      "\tCross-entropy: 9.903\tAccuracy: 0.567\n",
      "\tCross-entropy: 9.803\tAccuracy: 0.571\n",
      "\tCross-entropy: 9.764\tAccuracy: 0.573\n",
      "\tCross-entropy: 9.914\tAccuracy: 0.567\n",
      "\tCross-entropy: 9.953\tAccuracy: 0.566\n",
      "\tCross-entropy: 9.654\tAccuracy: 0.578\n",
      "\tCross-entropy: 9.623\tAccuracy: 0.580\n",
      "\tCross-entropy: 9.598\tAccuracy: 0.581\n",
      "\tCross-entropy: 9.550\tAccuracy: 0.583\n",
      "\tCross-entropy: 9.567\tAccuracy: 0.583\n",
      "\tCross-entropy: 9.740\tAccuracy: 0.576\n",
      "\tCross-entropy: 9.539\tAccuracy: 0.584\n",
      "\tCross-entropy: 9.437\tAccuracy: 0.588\n",
      "\tCross-entropy: 9.458\tAccuracy: 0.588\n",
      "\tCross-entropy: 9.403\tAccuracy: 0.590\n",
      "\tCross-entropy: 9.387\tAccuracy: 0.591\n",
      "\tCross-entropy: 9.441\tAccuracy: 0.588\n",
      "\tCross-entropy: 9.477\tAccuracy: 0.587\n",
      "\tCross-entropy: 9.348\tAccuracy: 0.592\n",
      "\tCross-entropy: 9.339\tAccuracy: 0.593\n",
      "\tCross-entropy: 9.347\tAccuracy: 0.593\n",
      "\tCross-entropy: 9.366\tAccuracy: 0.592\n",
      "\tCross-entropy: 9.397\tAccuracy: 0.590\n",
      "\tCross-entropy: 9.329\tAccuracy: 0.593\n",
      "\tCross-entropy: 9.316\tAccuracy: 0.594\n",
      "\tCross-entropy: 9.345\tAccuracy: 0.593\n",
      "\tCross-entropy: 9.279\tAccuracy: 0.596\n",
      "\tCross-entropy: 9.315\tAccuracy: 0.594\n",
      "\tCross-entropy: 9.262\tAccuracy: 0.596\n",
      "\tCross-entropy: 9.249\tAccuracy: 0.597\n",
      "\tCross-entropy: 9.251\tAccuracy: 0.597\n",
      "\tCross-entropy: 9.241\tAccuracy: 0.597\n",
      "\tCross-entropy: 9.230\tAccuracy: 0.597\n",
      "\tCross-entropy: 9.222\tAccuracy: 0.598\n",
      "\tCross-entropy: 9.218\tAccuracy: 0.598\n",
      "\tCross-entropy: 9.217\tAccuracy: 0.598\n",
      "\tCross-entropy: 9.202\tAccuracy: 0.599\n",
      "\tCross-entropy: 9.201\tAccuracy: 0.599\n",
      "\tCross-entropy: 9.206\tAccuracy: 0.599\n",
      "\tCross-entropy: 9.187\tAccuracy: 0.600\n",
      "\tCross-entropy: 9.181\tAccuracy: 0.600\n",
      "\tCross-entropy: 9.184\tAccuracy: 0.600\n",
      "\tCross-entropy: 9.181\tAccuracy: 0.600\n",
      "\tCross-entropy: 9.170\tAccuracy: 0.600\n",
      "\tCross-entropy: 9.187\tAccuracy: 0.600\n",
      "\tCross-entropy: 9.157\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.171\tAccuracy: 0.600\n",
      "\tCross-entropy: 9.167\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.144\tAccuracy: 0.602\n",
      "\tCross-entropy: 9.140\tAccuracy: 0.602\n",
      "\tCross-entropy: 9.137\tAccuracy: 0.602\n",
      "\tCross-entropy: 9.169\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.167\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.161\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.177\tAccuracy: 0.600\n",
      "\tCross-entropy: 9.173\tAccuracy: 0.600\n",
      "\tCross-entropy: 9.146\tAccuracy: 0.601\n",
      "\tCross-entropy: 9.137\tAccuracy: 0.602\n",
      "\tCross-entropy: 9.121\tAccuracy: 0.602\n",
      "\tCross-entropy: 9.120\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.131\tAccuracy: 0.602\n",
      "\tCross-entropy: 9.118\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.131\tAccuracy: 0.602\n",
      "\tCross-entropy: 9.114\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.106\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.104\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.100\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.112\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.119\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.083\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.089\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.100\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.092\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.077\tAccuracy: 0.605\n",
      "\tCross-entropy: 9.070\tAccuracy: 0.605\n",
      "\tCross-entropy: 9.094\tAccuracy: 0.604\n",
      "\tCross-entropy: 9.108\tAccuracy: 0.603\n",
      "\tCross-entropy: 9.055\tAccuracy: 0.606\n",
      "\tCross-entropy: 9.068\tAccuracy: 0.605\n",
      "\tCross-entropy: 9.071\tAccuracy: 0.605\n",
      "\tCross-entropy: 9.056\tAccuracy: 0.606\n",
      "\tCross-entropy: 9.072\tAccuracy: 0.605\n",
      "\tCross-entropy: 9.047\tAccuracy: 0.606\n",
      "\tCross-entropy: 9.044\tAccuracy: 0.606\n",
      "\tCross-entropy: 9.041\tAccuracy: 0.606\n"
     ]
    }
   ],
   "source": [
    "input_glimpse_flat = input_glimpses.reshape(-1, 28*28*3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    indices = range(len(input_glimpses))\n",
    "    for epoch in range(100):\n",
    "        batches = minibatch(indices, 10000, len(indices))\n",
    "\n",
    "        for index_batch in batches:\n",
    "            glimpses = input_glimpses[index_batch]\n",
    "            glimpses = glimpses.reshape(-1, 28*28*3)\n",
    "            gazes = input_gazes[index_batch]\n",
    "            output = outputs[index_batch]\n",
    "#             print(\"Nonbraking: {:.0f}\\tBraking: {:.0f}\".format(output[:, 0].sum(), output[:, 1].sum()))\n",
    "            sess.run(optimizer, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "        ce = sess.run(cross_entropy, feed_dict={image_input: input_glimpse_flat, gaze_input: input_gazes, y_: outputs})\n",
    "        acc = sess.run(accuracy, feed_dict={image_input: input_glimpse_flat, gaze_input: input_gazes, y_: outputs})\n",
    "#         pred = sess.run(y, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "#         num_pred_nonbrake = pred[:, 0].sum()\n",
    "#         num_pred_brake = pred[:, 1].sum()\n",
    "#         print(\"\\tNon-brake: {:.0f}\\tBrake: {:.0f}\".format(num_pred_nonbrake, num_pred_brake))\n",
    "        print(\"\\tCross-entropy: {:.3f}\\tAccuracy: {:.3f}\".format(ce, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feedforward Neural Net\n",
    "Our next model will be a feedforward neural network with a ReLu activation function with a 1024 neuron hidden layer. We'll train it using the same methodology as the logistic regression model (SGD and cross-entropy).\n",
    "\n",
    "![alt text](http://cs231n.github.io/assets/nn1/neural_net.jpeg \"Neural Network\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Create neural network for brake classification with 28x28x3 image input.\"\"\"\n",
    "# Create placeholders for inputs that will be placed via batches\n",
    "image_input = tf.placeholder(tf.float32, [None, 28*28*3], name=\"image\")\n",
    "gaze_input = tf.placeholder(tf.float32, [None, 2], name=\"gaze\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 2], name=\"output\")\n",
    "\n",
    "image_weights = tf.Variable(tf.truncated_normal([28*28*3, 1024], stddev=1), name=\"image_weights\")\n",
    "image_hidden_weights = tf.Variable(tf.truncated_normal([1024, 2], stddev=1), name=\"image_hidden_weights\")\n",
    "\n",
    "gaze_weights = tf.Variable(tf.truncated_normal([2, 1024], stddev=1), name=\"gaze_weights\")\n",
    "gaze_hidden_weights = tf.Variable(tf.truncated_normal([1024, 2], stddev=1), name=\"gaze_hidden_weights\")\n",
    "\n",
    "image_bias = tf.Variable(tf.truncated_normal([1024], stddev=1), name=\"image_bias\")\n",
    "image_hidden_bias = tf.Variable(tf.truncated_normal([2], stddev=1), name=\"image_hidden_bias\")\n",
    "\n",
    "gaze_bias = tf.Variable(tf.truncated_normal([1024], stddev=1), name=\"gaze_bias\")\n",
    "gaze_hidden_bias = tf.Variable(tf.truncated_normal([2], stddev=1), name=\"gaze_hidden_bias\")\n",
    "\n",
    "image_input_layer = tf.matmul(image_input, image_weights) + image_bias\n",
    "image_hidden_layer = tf.matmul(tf.nn.relu(image_input_layer), image_hidden_weights) + image_hidden_bias\n",
    "\n",
    "gaze_input_layer = tf.matmul(gaze_input, gaze_weights) + gaze_bias\n",
    "gaze_hidden_layer = tf.matmul(tf.nn.relu(gaze_input_layer), gaze_hidden_weights) + gaze_hidden_bias\n",
    "\n",
    "logits = tf.mul(tf.add(image_hidden_layer, gaze_hidden_layer), 0.5)\n",
    "y = tf.nn.softmax(logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.reduce_sum(-y_*tf.log(tf.clip_by_value(y, 1e-10,1.0)),reduction_indices=[1]))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "# initialization of variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Define computations for accuracy calculation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_glimpse_flat = input_glimpses.reshape(-1, 28*28*3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    indices = range(len(input_glimpses))\n",
    "    for epoch in range(100):\n",
    "        batches = minibatch(indices, 1000, len(indices))\n",
    "\n",
    "        for index_batch in batches:\n",
    "            glimpses = input_glimpses[index_batch]\n",
    "            glimpses = glimpses.reshape(-1, 28*28*3)\n",
    "            gazes = input_gazes[index_batch]\n",
    "            output = outputs[index_batch]\n",
    "#             print(\"Nonbraking: {:.0f}\\tBraking: {:.0f}\".format(output[:, 0].sum(), output[:, 1].sum()))\n",
    "            sess.run(optimizer, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "        ce = sess.run(cross_entropy, feed_dict={image_input: input_glimpse_flat, gaze_input: input_gazes, y_: outputs})\n",
    "        acc = sess.run(accuracy, feed_dict={image_input: input_glimpse_flat, gaze_input: input_gazes, y_: outputs})\n",
    "#         pred = sess.run(y, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "#         num_pred_nonbrake = pred[:, 0].sum()\n",
    "#         num_pred_brake = pred[:, 1].sum()\n",
    "#         print(\"\\tNon-brake: {:.0f}\\tBrake: {:.0f}\".format(num_pred_nonbrake, num_pred_brake))\n",
    "        print(\"\\tCross-entropy: {:.3f}\\tAccuracy: {:.3f}\".format(ce, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "The feedforward network's results were promising! With just one hidden layer, we brought the initial training cross-entropy down from 11 to 10 (still bad, but it's getting better). Now, we'll try a wide-and-deep network where the deep part is a convnet on the image and the wide portion processes the driver gaze's coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define some helper methods that will abstract variable initialization and layer definitions\n",
    "def weight_variable(shape, mean=0.0, wd=None):\n",
    "    initial = tf.truncated_normal(shape, mean=mean, stddev=0.1)\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)  # Store losses in a collection\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape, wd=None):\n",
    "    initial = tf.constant(0.5, shape=shape)\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)  # Store losses in a collection\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, name='conv'):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name=name)\n",
    "\n",
    "def max_pool_2x2(x, name='max_pool_2x2'):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define convolutional computation graph\n",
    "\n",
    "# Placeholders for parameters: image, gaze (x, y), output, dropout probability\n",
    "image_input = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"image\")\n",
    "gaze_input = tf.placeholder(tf.float32, [None, 2], name=\"gaze\")\n",
    "brake_seq_input = tf.placeholder(tf.float32, [None, 3*2], name=\"brake_sequence\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 2], name=\"output\")\n",
    "keep_prob = 0.5\n",
    "\n",
    "\n",
    "# Convolutional net for image processing\n",
    "\n",
    "# First layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])  # 5x5x1 filter with 32 features\n",
    "b_conv1 = bias_variable([32])             # Bias for each filter\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(image_input, W_conv1) + b_conv1, name='conv1')\n",
    "h_pool1 = max_pool_2x2(h_conv1, name='pool1')\n",
    "\n",
    "# Second layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, name='conv2')\n",
    "h_pool2 = max_pool_2x2(h_conv2, name='pool2')\n",
    "\n",
    "# Fully-connected layer hidden layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, 'hidden1')\n",
    "\n",
    "# Add dropout for fully-connected hidden layer\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Final logits\n",
    "W_fc2 = weight_variable([1024, 2])\n",
    "b_fc2 = bias_variable([2])\n",
    "\n",
    "image_logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "# Logistic regression for human gaze\n",
    "W_g = weight_variable([2, 2])\n",
    "b_g = bias_variable([2])\n",
    "\n",
    "gaze_logits = tf.matmul(gaze_input, W_g) + b_g\n",
    "\n",
    "\n",
    "# Logistic regression for braking sequence\n",
    "W_bs = weight_variable([3*2, 2])\n",
    "b_bs = bias_variable([2])\n",
    "\n",
    "bs_logits = tf.matmul(brake_seq_input, W_bs) + b_bs\n",
    "\n",
    "# Weights for final logits\n",
    "image_logits_weights = weight_variable([2, 2], mean=0.3)\n",
    "gaze_logits_weights = weight_variable([2, 2], mean=0.3)\n",
    "bs_logits_weights = weight_variable([2, 2], mean=0.3)\n",
    "bias = bias_variable([2])\n",
    "\n",
    "# Combine logistic and convnet\n",
    "logits = tf.add(tf.matmul(image_logits, image_logits_weights) + tf.matmul(gaze_logits, gaze_logits_weights) + tf.matmul(bs_logits, bs_logits_weights), bias)\n",
    "y = tf.nn.softmax(logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.reduce_sum(-y_*tf.log(tf.clip_by_value(y, 1e-10,1.0)),reduction_indices=[1]))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "# initialization of variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Define computations for accuracy calculation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCross-entropy: 0.134\tAccuracy: 0.988\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4d53ffdb26d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                                  \u001b[0mgaze_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_gazes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                                                  \u001b[0mbrake_seq_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                                                                  y_: outputs[test_inds]})\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#         num_pred_nonbrake = pred[:, 0].sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CONVNET_FILE_NAME = \"model/convnet.ckpt\"\n",
    "with tf.Session() as sess:\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, CONVNET_FILE_NAME)\n",
    "    for epoch in range(100):\n",
    "#         batches = minibatch(training_inds, 10000, len(training_inds))\n",
    "#         print(\"Batches: {0}\".format(len(batches)))\n",
    "#         for batch_num, index_batch in enumerate(batches):\n",
    "#             print(\"\\tProcessing batch {0}\".format(batch_num + 1))\n",
    "#             glimpses = input_glimpses[index_batch, :, :, :1]\n",
    "#             gazes = input_gazes[index_batch]\n",
    "#             seq = sequences[index_batch]\n",
    "#             output = outputs[index_batch]\n",
    "# #             print(\"Nonbraking: {:.0f}\\tBraking: {:.0f}\".format(output[:, 0].sum(), output[:, 1].sum()))\n",
    "#             sess.run(optimizer, feed_dict={image_input: glimpses, gaze_input: gazes, brake_seq_input: seq, y_: output})\n",
    "\n",
    "#         save_path = saver.save(sess, CONVNET_FILE_NAME)\n",
    "#         print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "        # Calculate cross-entropy, accuracy on last 10,000\n",
    "        ce, acc = sess.run([cross_entropy, accuracy], feed_dict={image_input: input_glimpses[test_inds, :, :, :1],\n",
    "                                                                 gaze_input: input_gazes[test_inds],\n",
    "                                                                 brake_seq_input: sequences[test_inds],\n",
    "                                                                 y_: outputs[test_inds]})\n",
    "\n",
    "#         num_pred_nonbrake = pred[:, 0].sum()\n",
    "#         num_pred_brake = pred[:, 1].sum()\n",
    "#         print(\"\\tNon-brake: {:.0f}\\tBrake: {:.0f}\".format(num_pred_nonbrake, num_pred_brake))\n",
    "        print(\"\\tCross-entropy: {:.3f}\\tAccuracy: {:.3f}\".format(ce, acc))\n",
    "#                 if index == 67:\n",
    "#                     fig = pylab.figure()\n",
    "#                     pylab.imshow(this_glimpse)\n",
    "#                     pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
