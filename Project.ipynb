{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autonomous Braking\n",
    "\n",
    "## Ethan Petersen, Josh Laesch, Ben Wong\n",
    "\n",
    "### The Dream Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as pylab\n",
    "import imageio\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imageio.core.util import asarray as imgToArr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "We perform some preprocessing on the data before feeding into the models. Namely, we will split the data into training, validation, and tests sets that will have equivalent amounts of braking and nonbraking frames. Note: frames 80,000 to 100,000 correspond to congested city center traffic following a large truck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# videoFile = './data/driving.avi'\n",
    "# vid = imageio.get_reader(videoFile,  'ffmpeg')\n",
    "\n",
    "# Columns: Frame, Brake, GazeX, GazeY\n",
    "dataFile = './data/cleaned_data.csv'\n",
    "df = pd.read_csv(dataFile, delimiter='\\t')\n",
    "\n",
    "brake = df[df['Brake'] > 0]\n",
    "nonbrake = df[df['Brake'] == 0]\n",
    "nonbrake = nonbrake[:len(brake)]  # Braking is far fewer than nonbraking, so trim down\n",
    "df = pd.concat([brake, nonbrake])\n",
    "df = df.drop(df[df['GazeX'] < 0].index)\n",
    "df = df.drop(df[df['GazeY'] < 0].index)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)  # Resets the index to the usual 0, 1, 2, ...\n",
    "\n",
    "# One-hot encode brakes\n",
    "outputs = OneHotEncoder(sparse=False).fit_transform(df['Brake'].reshape(-1,1))  # column 0: no brake, column 1: brake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In training, we sometimes pull straight from the video and specify a stride length\n",
    "# but we have also stored 28x28x3 glimpses for each frame\n",
    "input_glimpses = np.zeros((80000, 28, 28, 3))\n",
    "input_gazes = np.zeros((80000, 2))\n",
    "outputs = np.zeros((80000, 2))\n",
    "for batch in range(1, 9):\n",
    "    file_name = \"data/glimpse_batchc_{0}.npz\".format(batch)\n",
    "    array = np.load(file_name)\n",
    "    input_glimpses[(batch - 1) * 10000: batch * 10000] = array['frames']\n",
    "    input_gazes[(batch - 1) * 10000: batch * 10000] = array['gazes']\n",
    "    outputs[(batch - 1) * 10000: batch * 10000] = array['braking']\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    if np.sum(outputs[i]) == 0:\n",
    "        outputs[i] = [1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper methods\n",
    "We define some helper functions: `minibatch` will get batches of data for training via SGD, `get_glimpses` processes a list of images and cuts out small glimpses based on a list of center coordinates, and `get_glimpse` performs the cutting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def minibatch(data, batch_size, data_size):\n",
    "    \"\"\"Generates a minibatch from the given data and parameters.\"\"\"\n",
    "    randomized = np.random.permutation(data)\n",
    "    batches = []\n",
    "    num_batches = 0\n",
    "    while num_batches * batch_size < data_size:\n",
    "        new_batch = randomized[num_batches * batch_size:(num_batches + 1) * batch_size]\n",
    "        batches.append(new_batch)\n",
    "        num_batches += 1\n",
    "    return batches\n",
    "\n",
    "def get_glimpses(images, coords):\n",
    "    \"\"\"Gets a batch of glimpses.\"\"\"\n",
    "    arr = []\n",
    "    for img, coord in zip(images, coords):\n",
    "        arr.append(get_glimpse(img, coord[0], coord[1]))\n",
    "    return np.array(arr)\n",
    "\n",
    "def get_glimpse(image, x, y, stride=14):\n",
    "    \"\"\"Returns a subsection (glimpse) of the image centered on the given point.\"\"\"\n",
    "    x = int(x)  # Force to int\n",
    "    y = int(y)  # Force to int\n",
    "    min_x = x - stride\n",
    "    max_x = x + stride\n",
    "    \n",
    "    min_y = y - stride\n",
    "    max_y = y + stride\n",
    "    image_glimpse = image[min_y:max_y, min_x:max_x, :]  # NOTE: row, column, RGB\n",
    "#     image_glimpse = image[min_y:max_y, min_x:max_x, 0]  # NOTE: row, column, RGB; everything is greyscale; flatten RGB layer\n",
    "    return imgToArr(image_glimpse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "This first model is the basic logistic regression, which we'll train using SGD. With this model, we can achieve 60% accuracy on the training set in 100 epochs.\n",
    " \n",
    "![alt text](https://www.tensorflow.org/versions/r0.9/images/softmax-regression-scalargraph.png \"Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Create a logistic regression model for brake classification with 28x28x3 image input.\"\"\"\n",
    "# Create placeholders for inputs that will be placed via batches\n",
    "image_input = tf.placeholder(tf.float32, [None, 28*28*3], name=\"image\")\n",
    "gaze_input = tf.placeholder(tf.float32, [None, 2], name=\"gaze\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 2], name=\"output\")\n",
    "\n",
    "image_weights = tf.Variable(tf.truncated_normal([28*28*3, 2], stddev=1), name=\"image_weights\")\n",
    "gaze_weights = tf.Variable(tf.truncated_normal([2, 2], stddev=1), name=\"gaze_weights\")\n",
    "\n",
    "image_bias = tf.Variable(tf.truncated_normal([2], stddev=1), name=\"image_bias\")\n",
    "gaze_bias = tf.Variable(tf.truncated_normal([2], stddev=1), name=\"gaze_bias\")\n",
    "\n",
    "image_logits = tf.matmul(image_input, image_weights) + image_bias\n",
    "gaze_logits = tf.matmul(gaze_input, gaze_weights) + gaze_bias\n",
    "\n",
    "logits = tf.mul(tf.add(image_logits, gaze_logits), 0.5)\n",
    "y = tf.nn.softmax(logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.reduce_sum(-y_*tf.log(tf.clip_by_value(y, 1e-10,1.0)),reduction_indices=[1]))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "# initialization of variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Define computations for accuracy calculation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_glimpse_flat = input_glimpses.reshape(-1, 28*28*3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    indices = range(len(input_glimpses))\n",
    "    for epoch in range(100):\n",
    "        batches = minibatch(indices, 10000, len(indices))\n",
    "\n",
    "        for index_batch in batches:\n",
    "            glimpses = input_glimpses[index_batch]\n",
    "            glimpses = glimpses.reshape(-1, 28*28*3)\n",
    "            gazes = input_gazes[index_batch]\n",
    "            output = outputs[index_batch]\n",
    "#             print(\"Nonbraking: {:.0f}\\tBraking: {:.0f}\".format(output[:, 0].sum(), output[:, 1].sum()))\n",
    "            sess.run(optimizer, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "        ce = sess.run(cross_entropy, feed_dict={image_input: input_glimpse_flat, gaze_input: input_gazes, y_: outputs})\n",
    "        acc = sess.run(accuracy, feed_dict={image_input: input_glimpse_flat, gaze_input: input_gazes, y_: outputs})\n",
    "#         pred = sess.run(y, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "#         num_pred_nonbrake = pred[:, 0].sum()\n",
    "#         num_pred_brake = pred[:, 1].sum()\n",
    "#         print(\"\\tNon-brake: {:.0f}\\tBrake: {:.0f}\".format(num_pred_nonbrake, num_pred_brake))\n",
    "        print(\"\\tCross-entropy: {:.3f}\\tAccuracy: {:.3f}\".format(ce, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feedforward Neural Net\n",
    "Our next model will be a feedforward neural network with a ReLu activation function with a 1024 neuron hidden layer. We'll train it using the same methodology as the logistic regression model (SGD and cross-entropy).\n",
    "\n",
    "![alt text](http://cs231n.github.io/assets/nn1/neural_net.jpeg \"Neural Network\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Create neural network for brake classification with 28x28x3 image input.\"\"\"\n",
    "# Create placeholders for inputs that will be placed via batches\n",
    "image_input = tf.placeholder(tf.float32, [None, 28*28*3], name=\"image\")\n",
    "gaze_input = tf.placeholder(tf.float32, [None, 2], name=\"gaze\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 2], name=\"output\")\n",
    "\n",
    "image_weights = tf.Variable(tf.truncated_normal([28*28*3, 1024], stddev=1), name=\"image_weights\")\n",
    "image_hidden_weights = tf.Variable(tf.truncated_normal([1024, 2], stddev=1), name=\"image_hidden_weights\")\n",
    "\n",
    "gaze_weights = tf.Variable(tf.truncated_normal([2, 1024], stddev=1), name=\"gaze_weights\")\n",
    "gaze_hidden_weights = tf.Variable(tf.truncated_normal([1024, 2], stddev=1), name=\"gaze_hidden_weights\")\n",
    "\n",
    "image_bias = tf.Variable(tf.truncated_normal([1024], stddev=1), name=\"image_bias\")\n",
    "image_hidden_bias = tf.Variable(tf.truncated_normal([2], stddev=1), name=\"image_hidden_bias\")\n",
    "\n",
    "gaze_bias = tf.Variable(tf.truncated_normal([1024], stddev=1), name=\"gaze_bias\")\n",
    "gaze_hidden_bias = tf.Variable(tf.truncated_normal([2], stddev=1), name=\"gaze_hidden_bias\")\n",
    "\n",
    "image_input_layer = tf.matmul(image_input, image_weights) + image_bias\n",
    "image_hidden_layer = tf.matmul(tf.nn.relu(image_input_layer), image_hidden_weights) + image_hidden_bias\n",
    "\n",
    "gaze_input_layer = tf.matmul(gaze_input, gaze_weights) + gaze_bias\n",
    "gaze_hidden_layer = tf.matmul(tf.nn.relu(gaze_input_layer), gaze_hidden_weights) + gaze_hidden_bias\n",
    "\n",
    "logits = tf.mul(tf.add(image_hidden_layer, gaze_hidden_layer), 0.5)\n",
    "y = tf.nn.softmax(logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.reduce_sum(-y_*tf.log(tf.clip_by_value(y, 1e-10,1.0)),reduction_indices=[1]))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "# initialization of variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Define computations for accuracy calculation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_glimpse_flat = input_glimpses.reshape(-1, 28*28*3)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    indices = range(len(input_glimpses))\n",
    "    for epoch in range(100):\n",
    "        batches = minibatch(indices, 1000, len(indices))\n",
    "\n",
    "        for index_batch in batches:\n",
    "            glimpses = input_glimpses[index_batch]\n",
    "            glimpses = glimpses.reshape(-1, 28*28*3)\n",
    "            gazes = input_gazes[index_batch]\n",
    "            output = outputs[index_batch]\n",
    "#             print(\"Nonbraking: {:.0f}\\tBraking: {:.0f}\".format(output[:, 0].sum(), output[:, 1].sum()))\n",
    "            sess.run(optimizer, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "        ce = sess.run(cross_entropy, feed_dict={image_input: input_glimpse_flat, gaze_input: input_gazes, y_: outputs})\n",
    "        acc = sess.run(accuracy, feed_dict={image_input: input_glimpse_flat, gaze_input: input_gazes, y_: outputs})\n",
    "#         pred = sess.run(y, feed_dict={image_input: glimpses, gaze_input: gazes, y_: output})\n",
    "#         num_pred_nonbrake = pred[:, 0].sum()\n",
    "#         num_pred_brake = pred[:, 1].sum()\n",
    "#         print(\"\\tNon-brake: {:.0f}\\tBrake: {:.0f}\".format(num_pred_nonbrake, num_pred_brake))\n",
    "        print(\"\\tCross-entropy: {:.3f}\\tAccuracy: {:.3f}\".format(ce, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "The feedforward network's results were promising! With just one hidden layer, we brought the initial training cross-entropy down from 11 to 10 (still bad, but it's getting better). Now, we'll try a wide-and-deep network where the deep part is a convnet on the image and the wide portion processes the driver gaze's coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define some helper methods that will abstract variable initialization and layer definitions\n",
    "def weight_variable(shape, mean=0.0, wd=None):\n",
    "    initial = tf.truncated_normal(shape, mean=mean, stddev=0.1)\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)  # Store losses in a collection\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape, wd=None):\n",
    "    initial = tf.constant(0.5, shape=shape)\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)  # Store losses in a collection\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, name='conv'):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name=name)\n",
    "\n",
    "def max_pool_2x2(x, name='max_pool_2x2'):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "def get_previous_seq(seq, start, end):\n",
    "    arr = seq[max([0, start]):end]\n",
    "    if start < 0:\n",
    "        arr = np.append(np.zeros((abs(start),2)), arr, axis=0)\n",
    "    for i in range(len(arr)):\n",
    "        if np.sum(arr[i]) == 0:\n",
    "            arr[i] = [1, 0]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define convolutional computation graph\n",
    "\n",
    "# Placeholders for parameters: image, gaze (x, y), output, dropout probability\n",
    "image_input = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"image\")\n",
    "gaze_input = tf.placeholder(tf.float32, [None, 2], name=\"gaze\")\n",
    "brake_seq_input = tf.placeholder(tf.float32, [None, 3*2], name=\"brake_sequence\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 2], name=\"output\")\n",
    "keep_prob = 0.5\n",
    "\n",
    "\n",
    "# Convolutional net for image processing\n",
    "\n",
    "# First layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])  # 5x5x1 filter with 32 features\n",
    "b_conv1 = bias_variable([32])             # Bias for each filter\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(image_input, W_conv1) + b_conv1, name='conv1')\n",
    "h_pool1 = max_pool_2x2(h_conv1, name='pool1')\n",
    "\n",
    "# Second layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, name='conv2')\n",
    "h_pool2 = max_pool_2x2(h_conv2, name='pool2')\n",
    "\n",
    "# Fully-connected layer hidden layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, 'hidden1')\n",
    "\n",
    "# Add dropout for fully-connected hidden layer\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Final logits\n",
    "W_fc2 = weight_variable([1024, 2])\n",
    "b_fc2 = bias_variable([2])\n",
    "\n",
    "image_logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "# Logistic regression for human gaze\n",
    "W_g = weight_variable([2, 2])\n",
    "b_g = bias_variable([2])\n",
    "\n",
    "gaze_logits = tf.matmul(gaze_input, W_g) + b_g\n",
    "\n",
    "\n",
    "# Logistic regression for braking sequence\n",
    "W_bs = weight_variable([3*2, 2])\n",
    "b_bs = bias_variable([2])\n",
    "\n",
    "bs_logits = tf.matmul(brake_seq_input, W_bs) + b_bs\n",
    "\n",
    "# Weights for final logits\n",
    "image_logits_weights = weight_variable([2, 2], mean=0.3)\n",
    "gaze_logits_weights = weight_variable([2, 2], mean=0.3)\n",
    "bs_logits_weights = weight_variable([2, 2], mean=0.3)\n",
    "bias = bias_variable([2])\n",
    "\n",
    "# Combine logistic and convnet\n",
    "logits = tf.add(tf.matmul(image_logits, image_logits_weights) + tf.matmul(gaze_logits, gaze_logits_weights) + tf.matmul(bs_logits, bs_logits_weights), bias)\n",
    "y = tf.nn.softmax(logits)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.reduce_sum(-y_*tf.log(tf.clip_by_value(y, 1e-10,1.0)),reduction_indices=[1]))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "# initialization of variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Define computations for accuracy calculation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6)\n",
      "Nonbraking: 4960\tBraking: 5040\n",
      "\tNon-brake: 2898\tBrake: 7102\n",
      "\tCross-entropy: 8.435\tAccuracy: 0.609\n",
      "(10000, 6)\n",
      "Nonbraking: 4994\tBraking: 5006\n",
      "\tNon-brake: 9543\tBrake: 457\n",
      "\tCross-entropy: 10.712\tAccuracy: 0.527\n",
      "(10000, 6)\n",
      "Nonbraking: 4884\tBraking: 5116\n",
      "\tNon-brake: 9114\tBrake: 886\n",
      "\tCross-entropy: 10.428\tAccuracy: 0.537\n",
      "(10000, 6)\n",
      "Nonbraking: 4944\tBraking: 5056\n",
      "\tNon-brake: 5584\tBrake: 4416\n",
      "\tCross-entropy: 7.569\tAccuracy: 0.655\n",
      "(10000, 6)\n",
      "Nonbraking: 4936\tBraking: 5064\n",
      "\tNon-brake: 505\tBrake: 9495\n",
      "\tCross-entropy: 10.428\tAccuracy: 0.543\n",
      "(10000, 6)\n",
      "Nonbraking: 4952\tBraking: 5048\n",
      "\tNon-brake: 131\tBrake: 9869\n",
      "\tCross-entropy: 11.178\tAccuracy: 0.513\n",
      "(10000, 6)\n",
      "Nonbraking: 4940\tBraking: 5060\n",
      "\tNon-brake: 88\tBrake: 9912\n",
      "\tCross-entropy: 11.194\tAccuracy: 0.513\n",
      "(10000, 6)\n",
      "Nonbraking: 4927\tBraking: 5073\n",
      "\tNon-brake: 92\tBrake: 9908\n",
      "\tCross-entropy: 11.171\tAccuracy: 0.514\n",
      "(10000, 6)\n",
      "Nonbraking: 4885\tBraking: 5115\n",
      "\tNon-brake: 215\tBrake: 9785\n",
      "\tCross-entropy: 10.810\tAccuracy: 0.529\n",
      "(10000, 6)\n",
      "Nonbraking: 4879\tBraking: 5121\n",
      "\tNon-brake: 677\tBrake: 9323\n",
      "\tCross-entropy: 10.013\tAccuracy: 0.561\n",
      "(10000, 6)\n",
      "Nonbraking: 4997\tBraking: 5003\n",
      "\tNon-brake: 2304\tBrake: 7696\n",
      "\tCross-entropy: 8.314\tAccuracy: 0.630\n",
      "(10000, 6)\n",
      "Nonbraking: 4998\tBraking: 5002\n",
      "\tNon-brake: 5399\tBrake: 4601\n",
      "\tCross-entropy: 7.262\tAccuracy: 0.673\n",
      "(10000, 6)\n",
      "Nonbraking: 4928\tBraking: 5072\n",
      "\tNon-brake: 6767\tBrake: 3233\n",
      "\tCross-entropy: 8.385\tAccuracy: 0.623\n",
      "(10000, 6)\n",
      "Nonbraking: 4931\tBraking: 5069\n",
      "\tNon-brake: 6984\tBrake: 3016\n",
      "\tCross-entropy: 8.600\tAccuracy: 0.615\n",
      "(10000, 6)\n",
      "Nonbraking: 4905\tBraking: 5095\n",
      "\tNon-brake: 6370\tBrake: 3630\n",
      "\tCross-entropy: 8.053\tAccuracy: 0.640\n",
      "(10000, 6)\n",
      "Nonbraking: 5014\tBraking: 4986\n",
      "\tNon-brake: 5107\tBrake: 4893\n",
      "\tCross-entropy: 7.209\tAccuracy: 0.676\n",
      "(10000, 6)\n",
      "Nonbraking: 4975\tBraking: 5025\n",
      "\tNon-brake: 3402\tBrake: 6598\n",
      "\tCross-entropy: 7.155\tAccuracy: 0.679\n",
      "(10000, 6)\n",
      "Nonbraking: 4995\tBraking: 5005\n",
      "\tNon-brake: 2848\tBrake: 7152\n",
      "\tCross-entropy: 7.669\tAccuracy: 0.658\n",
      "(10000, 6)\n",
      "Nonbraking: 4878\tBraking: 5122\n",
      "\tNon-brake: 3503\tBrake: 6497\n",
      "\tCross-entropy: 7.012\tAccuracy: 0.687\n",
      "(10000, 6)\n",
      "Nonbraking: 4905\tBraking: 5095\n",
      "\tNon-brake: 4435\tBrake: 5565\n",
      "\tCross-entropy: 6.668\tAccuracy: 0.700\n",
      "(10000, 6)\n",
      "Nonbraking: 4984\tBraking: 5016\n",
      "\tNon-brake: 5300\tBrake: 4700\n",
      "\tCross-entropy: 6.928\tAccuracy: 0.686\n",
      "(10000, 6)\n",
      "Nonbraking: 4898\tBraking: 5102\n",
      "\tNon-brake: 5435\tBrake: 4565\n",
      "\tCross-entropy: 6.994\tAccuracy: 0.684\n",
      "(10000, 6)\n",
      "Nonbraking: 4890\tBraking: 5110\n",
      "\tNon-brake: 4885\tBrake: 5115\n",
      "\tCross-entropy: 6.669\tAccuracy: 0.698\n",
      "(10000, 6)\n",
      "Nonbraking: 5012\tBraking: 4988\n",
      "\tNon-brake: 4010\tBrake: 5990\n",
      "\tCross-entropy: 6.622\tAccuracy: 0.702\n",
      "(10000, 6)\n",
      "Nonbraking: 4880\tBraking: 5120\n",
      "\tNon-brake: 3129\tBrake: 6871\n",
      "\tCross-entropy: 6.879\tAccuracy: 0.691\n",
      "(10000, 6)\n",
      "Nonbraking: 4999\tBraking: 5001\n",
      "\tNon-brake: 3161\tBrake: 6839\n",
      "\tCross-entropy: 6.929\tAccuracy: 0.688\n",
      "(10000, 6)\n",
      "Nonbraking: 4941\tBraking: 5059\n",
      "\tNon-brake: 3698\tBrake: 6302\n",
      "\tCross-entropy: 6.594\tAccuracy: 0.703\n",
      "(10000, 6)\n",
      "Nonbraking: 4864\tBraking: 5136\n",
      "\tNon-brake: 4424\tBrake: 5576\n",
      "\tCross-entropy: 6.250\tAccuracy: 0.715\n",
      "(10000, 6)\n",
      "Nonbraking: 4975\tBraking: 5025\n",
      "\tNon-brake: 5549\tBrake: 4451\n",
      "\tCross-entropy: 6.371\tAccuracy: 0.710\n",
      "(10000, 6)\n",
      "Nonbraking: 5002\tBraking: 4998\n",
      "\tNon-brake: 5945\tBrake: 4055\n",
      "\tCross-entropy: 6.573\tAccuracy: 0.700\n",
      "(10000, 6)\n",
      "Nonbraking: 4878\tBraking: 5122\n",
      "\tNon-brake: 5563\tBrake: 4437\n",
      "\tCross-entropy: 6.219\tAccuracy: 0.713\n",
      "(10000, 6)\n",
      "Nonbraking: 4998\tBraking: 5002\n",
      "\tNon-brake: 4828\tBrake: 5172\n",
      "\tCross-entropy: 5.926\tAccuracy: 0.727\n",
      "(10000, 6)\n",
      "Nonbraking: 4938\tBraking: 5062\n",
      "\tNon-brake: 4158\tBrake: 5842\n",
      "\tCross-entropy: 6.027\tAccuracy: 0.722\n",
      "(10000, 6)\n",
      "Nonbraking: 4980\tBraking: 5020\n",
      "\tNon-brake: 3834\tBrake: 6166\n",
      "\tCross-entropy: 6.219\tAccuracy: 0.713\n",
      "(10000, 6)\n",
      "Nonbraking: 4951\tBraking: 5049\n",
      "\tNon-brake: 4019\tBrake: 5981\n",
      "\tCross-entropy: 5.730\tAccuracy: 0.733\n",
      "(10000, 6)\n",
      "Nonbraking: 4978\tBraking: 5022\n",
      "\tNon-brake: 5008\tBrake: 4992\n",
      "\tCross-entropy: 5.705\tAccuracy: 0.733\n",
      "(10000, 6)\n",
      "Nonbraking: 4904\tBraking: 5096\n",
      "\tNon-brake: 5528\tBrake: 4472\n",
      "\tCross-entropy: 5.889\tAccuracy: 0.723\n",
      "(10000, 6)\n",
      "Nonbraking: 4949\tBraking: 5051\n",
      "\tNon-brake: 5344\tBrake: 4656\n",
      "\tCross-entropy: 5.647\tAccuracy: 0.729\n",
      "(10000, 6)\n",
      "Nonbraking: 4883\tBraking: 5117\n",
      "\tNon-brake: 4830\tBrake: 5170\n",
      "\tCross-entropy: 5.415\tAccuracy: 0.738\n",
      "(10000, 6)\n",
      "Nonbraking: 4954\tBraking: 5046\n",
      "\tNon-brake: 4260\tBrake: 5740\n",
      "\tCross-entropy: 5.341\tAccuracy: 0.736\n",
      "(10000, 6)\n",
      "Nonbraking: 5006\tBraking: 4994\n",
      "\tNon-brake: 4569\tBrake: 5431\n",
      "\tCross-entropy: 5.237\tAccuracy: 0.736\n",
      "(10000, 6)\n",
      "Nonbraking: 4911\tBraking: 5089\n",
      "\tNon-brake: 4710\tBrake: 5290\n",
      "\tCross-entropy: 5.096\tAccuracy: 0.742\n",
      "(10000, 6)\n",
      "Nonbraking: 4959\tBraking: 5041\n",
      "\tNon-brake: 5144\tBrake: 4856\n",
      "\tCross-entropy: 4.990\tAccuracy: 0.740\n",
      "(10000, 6)\n",
      "Nonbraking: 4922\tBraking: 5078\n",
      "\tNon-brake: 4895\tBrake: 5105\n",
      "\tCross-entropy: 4.729\tAccuracy: 0.745\n",
      "(10000, 6)\n",
      "Nonbraking: 5007\tBraking: 4993\n",
      "\tNon-brake: 4079\tBrake: 5921\n",
      "\tCross-entropy: 4.662\tAccuracy: 0.734\n",
      "(10000, 6)\n",
      "Nonbraking: 4879\tBraking: 5121\n",
      "\tNon-brake: 4370\tBrake: 5630\n",
      "\tCross-entropy: 4.215\tAccuracy: 0.740\n",
      "(10000, 6)\n",
      "Nonbraking: 4902\tBraking: 5098\n",
      "\tNon-brake: 5313\tBrake: 4687\n",
      "\tCross-entropy: 3.980\tAccuracy: 0.745\n",
      "(10000, 6)\n",
      "Nonbraking: 4951\tBraking: 5049\n",
      "\tNon-brake: 4980\tBrake: 5020\n",
      "\tCross-entropy: 3.208\tAccuracy: 0.751\n",
      "(10000, 6)\n",
      "Nonbraking: 4928\tBraking: 5072\n",
      "\tNon-brake: 3929\tBrake: 6071\n",
      "\tCross-entropy: 3.021\tAccuracy: 0.720\n",
      "(10000, 6)\n",
      "Nonbraking: 4923\tBraking: 5077\n",
      "\tNon-brake: 5703\tBrake: 4297\n",
      "\tCross-entropy: 2.410\tAccuracy: 0.730\n",
      "(10000, 6)\n",
      "Nonbraking: 4909\tBraking: 5091\n",
      "\tNon-brake: 5430\tBrake: 4570\n",
      "\tCross-entropy: 1.993\tAccuracy: 0.698\n",
      "(10000, 6)\n",
      "Nonbraking: 5033\tBraking: 4967\n",
      "\tNon-brake: 4093\tBrake: 5907\n",
      "\tCross-entropy: 1.625\tAccuracy: 0.678\n",
      "(10000, 6)\n",
      "Nonbraking: 4880\tBraking: 5120\n",
      "\tNon-brake: 5666\tBrake: 4334\n",
      "\tCross-entropy: 1.322\tAccuracy: 0.664\n",
      "(10000, 6)\n",
      "Nonbraking: 4994\tBraking: 5006\n",
      "\tNon-brake: 5799\tBrake: 4201\n",
      "\tCross-entropy: 1.102\tAccuracy: 0.669\n",
      "(10000, 6)\n",
      "Nonbraking: 4901\tBraking: 5099\n",
      "\tNon-brake: 4767\tBrake: 5233\n",
      "\tCross-entropy: 0.925\tAccuracy: 0.679\n",
      "(10000, 6)\n",
      "Nonbraking: 4969\tBraking: 5031\n",
      "\tNon-brake: 4813\tBrake: 5187\n",
      "\tCross-entropy: 0.815\tAccuracy: 0.694\n",
      "(10000, 6)\n",
      "Nonbraking: 4972\tBraking: 5028\n",
      "\tNon-brake: 5583\tBrake: 4417\n",
      "\tCross-entropy: 0.704\tAccuracy: 0.713\n",
      "(10000, 6)\n",
      "Nonbraking: 4879\tBraking: 5121\n",
      "\tNon-brake: 5870\tBrake: 4130\n",
      "\tCross-entropy: 0.678\tAccuracy: 0.709\n",
      "(10000, 6)\n",
      "Nonbraking: 4976\tBraking: 5024\n",
      "\tNon-brake: 5961\tBrake: 4039\n",
      "\tCross-entropy: 0.606\tAccuracy: 0.729\n",
      "(10000, 6)\n",
      "Nonbraking: 4860\tBraking: 5140\n",
      "\tNon-brake: 5547\tBrake: 4453\n",
      "\tCross-entropy: 0.580\tAccuracy: 0.732\n",
      "(10000, 6)\n",
      "Nonbraking: 4926\tBraking: 5074\n",
      "\tNon-brake: 5154\tBrake: 4846\n",
      "\tCross-entropy: 0.558\tAccuracy: 0.736\n",
      "(10000, 6)\n",
      "Nonbraking: 5012\tBraking: 4988\n",
      "\tNon-brake: 4979\tBrake: 5021\n",
      "\tCross-entropy: 0.544\tAccuracy: 0.745\n",
      "(10000, 6)\n",
      "Nonbraking: 4990\tBraking: 5010\n",
      "\tNon-brake: 5027\tBrake: 4973\n",
      "\tCross-entropy: 0.529\tAccuracy: 0.749\n",
      "(10000, 6)\n",
      "Nonbraking: 4922\tBraking: 5078\n",
      "\tNon-brake: 5158\tBrake: 4842\n",
      "\tCross-entropy: 0.523\tAccuracy: 0.750\n",
      "(10000, 6)\n",
      "Nonbraking: 4932\tBraking: 5068\n",
      "\tNon-brake: 5314\tBrake: 4686\n",
      "\tCross-entropy: 0.526\tAccuracy: 0.749\n",
      "(10000, 6)\n",
      "Nonbraking: 4889\tBraking: 5111\n",
      "\tNon-brake: 5269\tBrake: 4731\n",
      "\tCross-entropy: 0.520\tAccuracy: 0.752\n",
      "(10000, 6)\n",
      "Nonbraking: 4899\tBraking: 5101\n",
      "\tNon-brake: 5069\tBrake: 4931\n",
      "\tCross-entropy: 0.510\tAccuracy: 0.768\n",
      "(10000, 6)\n",
      "Nonbraking: 4931\tBraking: 5069\n",
      "\tNon-brake: 4798\tBrake: 5202\n",
      "\tCross-entropy: 0.515\tAccuracy: 0.765\n",
      "(10000, 6)\n",
      "Nonbraking: 4925\tBraking: 5075\n",
      "\tNon-brake: 4595\tBrake: 5405\n",
      "\tCross-entropy: 0.508\tAccuracy: 0.768\n",
      "(10000, 6)\n",
      "Nonbraking: 4942\tBraking: 5058\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    indices = range(len(input_glimpses))\n",
    "    for epoch in range(100):\n",
    "        batches = minibatch(indices, 10000, len(indices))\n",
    "\n",
    "        for index_batch in batches:\n",
    "            glimpses = input_glimpses[index_batch, :, :, :1]\n",
    "            gazes = input_gazes[index_batch]\n",
    "#             seq = np.array([outputs[i-3:i, :] for i in index_batch])\n",
    "            seq = np.array([get_previous_seq(outputs, i-3, i) for i in index_batch])\n",
    "            seq = seq.reshape(-1, 3*2)\n",
    "            output = outputs[index_batch]\n",
    "            print(\"Nonbraking: {:.0f}\\tBraking: {:.0f}\".format(output[:, 0].sum(), output[:, 1].sum()))\n",
    "            sess.run(optimizer, feed_dict={image_input: glimpses, gaze_input: gazes, brake_seq_input: seq, y_: output})\n",
    "            ce, acc, pred = sess.run([cross_entropy, accuracy, y], feed_dict={image_input: glimpses, gaze_input: gazes, brake_seq_input: seq, y_: output})\n",
    "\n",
    "            num_pred_nonbrake = pred[:, 0].sum()\n",
    "            num_pred_brake = pred[:, 1].sum()\n",
    "            print(\"\\tNon-brake: {:.0f}\\tBrake: {:.0f}\".format(num_pred_nonbrake, num_pred_brake))\n",
    "            print(\"\\tCross-entropy: {:.3f}\\tAccuracy: {:.3f}\".format(ce, acc))\n",
    "#                 if index == 67:\n",
    "#                     fig = pylab.figure()\n",
    "#                     pylab.imshow(this_glimpse)\n",
    "#                     pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
